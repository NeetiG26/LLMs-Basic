{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffPYsaiza0PO"
      },
      "source": [
        "# Get started with LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRETn2MsZcj2"
      },
      "source": [
        "## Table of Content\n",
        "\n",
        "- Installation\n",
        "- Set-up OpenAI Key\n",
        "- Langchain Components\n",
        "  - Model\n",
        "  - Prompt Template\n",
        "  - Output Parser\n",
        "  - Chain\n",
        "  - Memory\n",
        "  - Agents\n",
        "  - Loaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAlcXsnEbFcn"
      },
      "source": [
        "> Note: Always a best practice is to change the Runtime\n",
        ">\n",
        "> Runtime -> Change Runtime type -> Select T4 GPU hardware accelerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGWPekniY2QT"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaOFbvD2a9lM",
        "outputId": "8b7502d3-075a-4016-d3ec-ea332587c2ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.0.312-py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 1.5 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting openai\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 20.3 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting SQLAlchemy<3,>=1.4\n",
            "  Downloading SQLAlchemy-2.0.21-cp39-cp39-macosx_11_0_arm64.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 30.1 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/neetigoenka/Library/Python/3.9/lib/python/site-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/neetigoenka/Library/Python/3.9/lib/python/site-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/neetigoenka/Library/Python/3.9/lib/python/site-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: numpy<2,>=1 in /Users/neetigoenka/Library/Python/3.9/lib/python/site-packages (from langchain) (1.24.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7\n",
            "  Downloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: anyio<4.0 in /Users/neetigoenka/Library/Python/3.9/lib/python/site-packages (from langchain) (3.6.2)\n",
            "Collecting langsmith<0.1.0,>=0.0.43\n",
            "  Downloading langsmith-0.0.43-py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 16.9 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting pydantic<3,>=1\n",
            "  Downloading pydantic-2.4.2-py3-none-any.whl (395 kB)\n",
            "\u001b[K     |████████████████████████████████| 395 kB 20.5 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /Users/neetigoenka/Library/Python/3.9/lib/python/site-packages (from langchain) (2.28.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /Users/neetigoenka/Library/Python/3.9/lib/python/site-packages (from langchain) (6.0)\n",
            "Collecting jsonpatch<2.0,>=1.33\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: tqdm in /Users/neetigoenka/Library/Python/3.9/lib/python/site-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/neetigoenka/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/neetigoenka/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/neetigoenka/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/neetigoenka/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/neetigoenka/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/neetigoenka/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: idna>=2.8 in /Users/neetigoenka/Library/Python/3.9/lib/python/site-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Users/neetigoenka/Library/Python/3.9/lib/python/site-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Collecting typing-inspect<1,>=0.4.0\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 13.2 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: jsonpointer>=1.9 in /Users/neetigoenka/Library/Python/3.9/lib/python/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.3)\n",
            "Requirement already satisfied: packaging>=17.0 in /Users/neetigoenka/Library/Python/3.9/lib/python/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.0)\n",
            "Collecting annotated-types>=0.4.0\n",
            "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
            "Collecting typing-extensions>=4.6.1\n",
            "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Collecting pydantic-core==2.10.1\n",
            "  Downloading pydantic_core-2.10.1-cp39-cp39-macosx_11_0_arm64.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 28.3 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /Users/neetigoenka/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/neetigoenka/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain) (1.26.15)\n",
            "Collecting mypy-extensions>=0.3.0\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: typing-extensions, pydantic-core, mypy-extensions, annotated-types, typing-inspect, pydantic, marshmallow, SQLAlchemy, langsmith, jsonpatch, dataclasses-json, openai, langchain\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.5.0\n",
            "    Uninstalling typing-extensions-4.5.0:\n",
            "      Successfully uninstalled typing-extensions-4.5.0\n",
            "Successfully installed SQLAlchemy-2.0.21 annotated-types-0.6.0 dataclasses-json-0.6.1 jsonpatch-1.33 langchain-0.0.312 langsmith-0.0.43 marshmallow-3.20.1 mypy-extensions-1.0.0 openai-0.28.1 pydantic-2.4.2 pydantic-core-2.10.1 typing-extensions-4.8.0 typing-inspect-0.9.0\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.2.1 is available.\n",
            "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip3 install langchain openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drEAVDigY4Eq"
      },
      "source": [
        "## Set-up OpenAI Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Z1lpHFGabYPm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import langchain\n",
        "import openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "wW3EvYfXbsIR"
      },
      "outputs": [],
      "source": [
        "openai_key = \"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\" #replace the key with your OpenAI Key\n",
        "os.environ['OPENAI_API_KEY'] = openai_key\n",
        "openai.api_key = os.environ['OPENAI_API_KEY']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sh99LS6nY8JG"
      },
      "source": [
        "## Model\n",
        "\n",
        "Chat models are a variation on language models. While chat models use language models under the hood, the interface they expose is a bit different. Rather than expose a \"text in, text out\" API, they expose an interface where \"chat messages\" are the inputs and outputs.\n",
        "\n",
        "![langchain-demo](https://python.langchain.com/assets/images/model_io-1f23a36233d7731e93576d6885da2750.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loJQ6YeWm7zy"
      },
      "source": [
        "> Note: Large Language Models (LLMs) are a core component of LangChain. LangChain does not serve its own LLMs, but rather provides a standard interface for interacting with many different LLMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "HswtN32MmKbV"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.llms import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFiBmSRcns6q"
      },
      "source": [
        "## LLMs\n",
        "\n",
        "Large Language Models (LLMs) are the models that take a text string as input, and return a text string as output.\n",
        "\n",
        "## Chat Models\n",
        "\n",
        "Chat Models are the models that are usually backed by a language model, but their APIs are more structured. Specifically, these models take a list of Chat Messages as input, and return a Chat Message."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "XAV0wLF_m-e3"
      },
      "outputs": [],
      "source": [
        "# To control the randomness and creativity of the generated, use temperature = 0.0 to 0.9\n",
        "chat = ChatOpenAI(temperature=0.0,max_tokens = 500)\n",
        "#chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "cGHfxoOZpAy5",
        "outputId": "07e21320-0762-45b0-e44f-dd97df4fb960"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'gpt-3.5-turbo'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat.model_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbhGQiBvofFN",
        "outputId": "1d56cde4-baf8-4149-e629-2a2ea679bbc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "500\n"
          ]
        }
      ],
      "source": [
        "print(chat.max_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "W_b9n4_2ol6C"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(temperature=0.0)\n",
        "#llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Bm37EtGsouU9",
        "outputId": "e58e49d7-d12c-4e73-de05-04b5c0e70432"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'text-davinci-003'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm.model_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ueq1OZF4ozTj",
        "outputId": "99017dce-bcb0-409b-939d-9feb20ca4b86"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm.max_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ow13c936Y-g1"
      },
      "source": [
        "## Prompt Template\n",
        "\n",
        "LangChain offers tools for crafting and utilizing prompt templates, which are pre-defined guidelines for generating prompts for language models, including instructions, few-shot examples, and context-specific questions suitable for various tasks. These templates aim to be model-agnostic, allowing for easy reuse across different language models. Generally, language models anticipate prompts to be either a string or a series of chat messages.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "hrMASx4Vpllj"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPiXKRlN3Pq0"
      },
      "source": [
        "> Let's create a prompt template, notice the variables inside {} curly brackets are treated as input variables and can be overriden based on the user input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXi6ZM0r42gY"
      },
      "source": [
        "## Please play around with your own prompts to understand how Prompt template works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "4wKxvL7sptvW"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"Generate a report \\\n",
        "highlighting the key findings from the {analysis_type} analysis. \\\n",
        "Here's a snippet of the analysis: \\\n",
        "```{analysis_snippet}```.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "N-ynHVs7qimq"
      },
      "outputs": [],
      "source": [
        "prompt_temp = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLsV_yZ0qsvk",
        "outputId": "f86e85c2-c845-401f-e6ca-ae3249582277"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['analysis_snippet', 'analysis_type'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['analysis_snippet', 'analysis_type'], template=\"Generate a report highlighting the key findings from the {analysis_type} analysis. Here's a snippet of the analysis: ```{analysis_snippet}```.\\n\"))])"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRLYzoXcqvNU",
        "outputId": "2c90b292-f22b-4ad5-9680-fb6d91137e02"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['analysis_snippet', 'analysis_type']"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt_temp.input_variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDqlS71dqzjM",
        "outputId": "8b95b8e1-14eb-4a43-85a8-bbc3fc07e5b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['analysis_snippet', 'analysis_type'], template=\"Generate a report highlighting the key findings from the {analysis_type} analysis. Here's a snippet of the analysis: ```{analysis_snippet}```.\\n\"))]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt_temp.messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "vWfOUrGDq6Ge"
      },
      "outputs": [],
      "source": [
        "sample_analysis_type = \"user_engagement\"\n",
        "\n",
        "sample_analysis_snippet = \"\"\"\n",
        "Over the past quarter, the user engagement metrics have shown promising growth.\n",
        "The number of active users has increased by 15%, with a notable uptick in daily logins.\n",
        "This increase is largely attributed to the successful launch of our new mobile app, which has received positive feedback from users.\n",
        "Additionally, user interactions with key features, such as commenting and sharing, have risen by 20%.\n",
        "This signifies a higher level of user participation and social engagement within our platform.\n",
        "We've also observed longer average session durations, indicating that users are spending more time exploring the content we offer.\n",
        "The engagement rate, calculated by dividing interactions by active users, has experienced a steady increase of 10% over the past two months.\n",
        "This suggests that our efforts to improve user experience and introduce new features are resonating well with our audience.\n",
        "\n",
        "Overall, the data suggests that our focus on enhancing user engagement through product improvements and feature launches is yielding positive results. As we continue to refine our strategies and innovate, we can expect further growth in user interactions and overall engagement.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYgqeTMx3jXf"
      },
      "source": [
        "> Using **format_messages** function we can override the input variables used in the Prompt template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "LSLfLrwOrFZB"
      },
      "outputs": [],
      "source": [
        "response_message = prompt_temp.format_messages(\n",
        "    analysis_type =sample_analysis_type,\n",
        "    analysis_snippet = sample_analysis_snippet\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k87S2-MeRdlM",
        "outputId": "60d1d0f9-abcb-4107-c81a-c31ec07e219b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content=\"Generate a report highlighting the key findings from the user_engagement analysis. Here's a snippet of the analysis: ```\\nOver the past quarter, the user engagement metrics have shown promising growth.\\nThe number of active users has increased by 15%, with a notable uptick in daily logins.\\nThis increase is largely attributed to the successful launch of our new mobile app, which has received positive feedback from users.\\nAdditionally, user interactions with key features, such as commenting and sharing, have risen by 20%.\\nThis signifies a higher level of user participation and social engagement within our platform.\\nWe've also observed longer average session durations, indicating that users are spending more time exploring the content we offer.\\nThe engagement rate, calculated by dividing interactions by active users, has experienced a steady increase of 10% over the past two months.\\nThis suggests that our efforts to improve user experience and introduce new features are resonating well with our audience.\\n\\nOverall, the data suggests that our focus on enhancing user engagement through product improvements and feature launches is yielding positive results. As we continue to refine our strategies and innovate, we can expect further growth in user interactions and overall engagement.\\n```.\\n\")]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response_message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "AjGAcGrVtnZ2",
        "outputId": "5b590c85-2fed-41dd-b703-b2de0be1c76b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Generate a report highlighting the key findings from the user_engagement analysis. Here's a snippet of the analysis: ```\\nOver the past quarter, the user engagement metrics have shown promising growth.\\nThe number of active users has increased by 15%, with a notable uptick in daily logins.\\nThis increase is largely attributed to the successful launch of our new mobile app, which has received positive feedback from users.\\nAdditionally, user interactions with key features, such as commenting and sharing, have risen by 20%.\\nThis signifies a higher level of user participation and social engagement within our platform.\\nWe've also observed longer average session durations, indicating that users are spending more time exploring the content we offer.\\nThe engagement rate, calculated by dividing interactions by active users, has experienced a steady increase of 10% over the past two months.\\nThis suggests that our efforts to improve user experience and introduce new features are resonating well with our audience.\\n\\nOverall, the data suggests that our focus on enhancing user engagement through product improvements and feature launches is yielding positive results. As we continue to refine our strategies and innovate, we can expect further growth in user interactions and overall engagement.\\n```.\\n\""
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response_message[0].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "FIIb7Hugre6L"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
            "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
            "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
            "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
            "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 10.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n"
          ]
        },
        {
          "ename": "RateLimitError",
          "evalue": "You exceeded your current quota, please check your plan and billing details.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m report \u001b[39m=\u001b[39m chat(response_message) \u001b[39m#chatopenai model-GPT3.5 turbo\u001b[39;00m\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain/chat_models/base.py:608\u001b[0m, in \u001b[0;36mBaseChatModel.__call__\u001b[0;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[1;32m    602\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    603\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    606\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    607\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BaseMessage:\n\u001b[0;32m--> 608\u001b[0m     generation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m    609\u001b[0m         [messages], stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    610\u001b[0m     )\u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m    611\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(generation, ChatGeneration):\n\u001b[1;32m    612\u001b[0m         \u001b[39mreturn\u001b[39;00m generation\u001b[39m.\u001b[39mmessage\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain/chat_models/base.py:359\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n\u001b[1;32m    358\u001b[0m             run_managers[i]\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 359\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    360\u001b[0m flattened_outputs \u001b[39m=\u001b[39m [\n\u001b[1;32m    361\u001b[0m     LLMResult(generations\u001b[39m=\u001b[39m[res\u001b[39m.\u001b[39mgenerations], llm_output\u001b[39m=\u001b[39mres\u001b[39m.\u001b[39mllm_output)\n\u001b[1;32m    362\u001b[0m     \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results\n\u001b[1;32m    363\u001b[0m ]\n\u001b[1;32m    364\u001b[0m llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_llm_outputs([res\u001b[39m.\u001b[39mllm_output \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results])\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain/chat_models/base.py:349\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[39mfor\u001b[39;00m i, m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(messages):\n\u001b[1;32m    347\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m         results\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 349\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_with_cache(\n\u001b[1;32m    350\u001b[0m                 m,\n\u001b[1;32m    351\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    352\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[i] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    353\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    354\u001b[0m             )\n\u001b[1;32m    355\u001b[0m         )\n\u001b[1;32m    356\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    357\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain/chat_models/base.py:501\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    498\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    499\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 501\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    502\u001b[0m         messages, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    503\u001b[0m     )\n\u001b[1;32m    504\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    505\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(messages, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain/chat_models/openai.py:360\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m message_dicts, params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    359\u001b[0m params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs}\n\u001b[0;32m--> 360\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompletion_with_retry(\n\u001b[1;32m    361\u001b[0m     messages\u001b[39m=\u001b[39;49mmessage_dicts, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams\n\u001b[1;32m    362\u001b[0m )\n\u001b[1;32m    363\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_chat_result(response)\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain/chat_models/openai.py:299\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    296\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 299\u001b[0m \u001b[39mreturn\u001b[39;00m _completion_with_retry(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[1;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tenacity/__init__.py:325\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    323\u001b[0m     retry_exc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry_error_cls(fut)\n\u001b[1;32m    324\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreraise:\n\u001b[0;32m--> 325\u001b[0m         \u001b[39mraise\u001b[39;00m retry_exc\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m    326\u001b[0m     \u001b[39mraise\u001b[39;00m retry_exc \u001b[39mfrom\u001b[39;00m \u001b[39mfut\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexception\u001b[39;00m()\n\u001b[1;32m    328\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwait:\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tenacity/__init__.py:158\u001b[0m, in \u001b[0;36mRetryError.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreraise\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mNoReturn:\n\u001b[1;32m    157\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_attempt\u001b[39m.\u001b[39mfailed:\n\u001b[0;32m--> 158\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlast_attempt\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    159\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\n",
            "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:438\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    437\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 438\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    440\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    442\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
            "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:390\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    389\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 390\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    391\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    393\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m         result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    383\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[1;32m    384\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain/chat_models/openai.py:297\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry.<locals>._completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    296\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> 297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/api_resources/abstract/engine_api_resource.py:155\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    131\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    139\u001b[0m ):\n\u001b[1;32m    140\u001b[0m     (\n\u001b[1;32m    141\u001b[0m         deployment_id,\n\u001b[1;32m    142\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    153\u001b[0m     )\n\u001b[0;32m--> 155\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    156\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    157\u001b[0m         url,\n\u001b[1;32m    158\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    159\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    160\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    161\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    162\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    165\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    166\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    167\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/api_requestor.py:299\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    279\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    280\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    288\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    289\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    290\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    291\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    297\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    298\u001b[0m     )\n\u001b[0;32m--> 299\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    300\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/api_requestor.py:710\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    703\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    704\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    707\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    708\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    709\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 710\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    711\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    712\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    713\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    714\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    715\u001b[0m         ),\n\u001b[1;32m    716\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    717\u001b[0m     )\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/api_requestor.py:775\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    773\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    774\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 775\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    776\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    777\u001b[0m     )\n\u001b[1;32m    778\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
            "\u001b[0;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details."
          ]
        }
      ],
      "source": [
        "report = chat(response_message) #chatopenai model-GPT3.5 turbo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "M3Wa3I8k1b7s"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
            "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
            "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
            "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
            "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 10.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n"
          ]
        },
        {
          "ename": "RateLimitError",
          "evalue": "You exceeded your current quota, please check your plan and billing details.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m report_llm \u001b[39m=\u001b[39m llm(response_message[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mcontent) \u001b[39m#OpenAI -llms - Text davinci-003\u001b[39;00m\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain/llms/base.py:867\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[0;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    860\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(prompt, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    861\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    862\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    863\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(prompt)\u001b[39m}\u001b[39;00m\u001b[39m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    864\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`generate` instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    865\u001b[0m     )\n\u001b[1;32m    866\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 867\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m    868\u001b[0m         [prompt],\n\u001b[1;32m    869\u001b[0m         stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    870\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    871\u001b[0m         tags\u001b[39m=\u001b[39;49mtags,\n\u001b[1;32m    872\u001b[0m         metadata\u001b[39m=\u001b[39;49mmetadata,\n\u001b[1;32m    873\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    874\u001b[0m     )\n\u001b[1;32m    875\u001b[0m     \u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m    876\u001b[0m     \u001b[39m.\u001b[39mtext\n\u001b[1;32m    877\u001b[0m )\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain/llms/base.py:647\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    633\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    634\u001b[0m         )\n\u001b[1;32m    635\u001b[0m     run_managers \u001b[39m=\u001b[39m [\n\u001b[1;32m    636\u001b[0m         callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    637\u001b[0m             dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m         )\n\u001b[1;32m    646\u001b[0m     ]\n\u001b[0;32m--> 647\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_helper(\n\u001b[1;32m    648\u001b[0m         prompts, stop, run_managers, \u001b[39mbool\u001b[39;49m(new_arg_supported), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    649\u001b[0m     )\n\u001b[1;32m    650\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n\u001b[1;32m    651\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_prompts) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain/llms/base.py:535\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n\u001b[1;32m    534\u001b[0m         run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 535\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    536\u001b[0m flattened_outputs \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m    537\u001b[0m \u001b[39mfor\u001b[39;00m manager, flattened_output \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(run_managers, flattened_outputs):\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain/llms/base.py:522\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate_helper\u001b[39m(\n\u001b[1;32m    513\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    514\u001b[0m     prompts: List[\u001b[39mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    519\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    520\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    521\u001b[0m         output \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 522\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    523\u001b[0m                 prompts,\n\u001b[1;32m    524\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    525\u001b[0m                 \u001b[39m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    526\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[\u001b[39m0\u001b[39;49m] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    527\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    528\u001b[0m             )\n\u001b[1;32m    529\u001b[0m             \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    530\u001b[0m             \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m    531\u001b[0m         )\n\u001b[1;32m    532\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    533\u001b[0m         \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain/llms/openai.py:401\u001b[0m, in \u001b[0;36mBaseOpenAI._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m     choices\u001b[39m.\u001b[39mappend(\n\u001b[1;32m    390\u001b[0m         {\n\u001b[1;32m    391\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m: generation\u001b[39m.\u001b[39mtext,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    398\u001b[0m         }\n\u001b[1;32m    399\u001b[0m     )\n\u001b[1;32m    400\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m     response \u001b[39m=\u001b[39m completion_with_retry(\n\u001b[1;32m    402\u001b[0m         \u001b[39mself\u001b[39;49m, prompt\u001b[39m=\u001b[39;49m_prompts, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams\n\u001b[1;32m    403\u001b[0m     )\n\u001b[1;32m    404\u001b[0m     choices\u001b[39m.\u001b[39mextend(response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    405\u001b[0m     update_token_usage(_keys, response, token_usage)\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain/llms/openai.py:115\u001b[0m, in \u001b[0;36mcompletion_with_retry\u001b[0;34m(llm, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    112\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    113\u001b[0m     \u001b[39mreturn\u001b[39;00m llm\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 115\u001b[0m \u001b[39mreturn\u001b[39;00m _completion_with_retry(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[1;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tenacity/__init__.py:325\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    323\u001b[0m     retry_exc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry_error_cls(fut)\n\u001b[1;32m    324\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreraise:\n\u001b[0;32m--> 325\u001b[0m         \u001b[39mraise\u001b[39;00m retry_exc\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m    326\u001b[0m     \u001b[39mraise\u001b[39;00m retry_exc \u001b[39mfrom\u001b[39;00m \u001b[39mfut\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexception\u001b[39;00m()\n\u001b[1;32m    328\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwait:\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tenacity/__init__.py:158\u001b[0m, in \u001b[0;36mRetryError.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreraise\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mNoReturn:\n\u001b[1;32m    157\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_attempt\u001b[39m.\u001b[39mfailed:\n\u001b[0;32m--> 158\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlast_attempt\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    159\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\n",
            "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:438\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    437\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 438\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    440\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    442\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
            "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:390\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    389\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 390\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    391\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    393\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m         result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    383\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[1;32m    384\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain/llms/openai.py:113\u001b[0m, in \u001b[0;36mcompletion_with_retry.<locals>._completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    112\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m llm\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/api_resources/completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/api_resources/abstract/engine_api_resource.py:155\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    131\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    139\u001b[0m ):\n\u001b[1;32m    140\u001b[0m     (\n\u001b[1;32m    141\u001b[0m         deployment_id,\n\u001b[1;32m    142\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    153\u001b[0m     )\n\u001b[0;32m--> 155\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    156\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    157\u001b[0m         url,\n\u001b[1;32m    158\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    159\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    160\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    161\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    162\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    165\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    166\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    167\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/api_requestor.py:299\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    279\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    280\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    288\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    289\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    290\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    291\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    297\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    298\u001b[0m     )\n\u001b[0;32m--> 299\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    300\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/api_requestor.py:710\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    703\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    704\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    707\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    708\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    709\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 710\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    711\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    712\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    713\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    714\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    715\u001b[0m         ),\n\u001b[1;32m    716\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    717\u001b[0m     )\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/api_requestor.py:775\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    773\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    774\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 775\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    776\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    777\u001b[0m     )\n\u001b[1;32m    778\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
            "\u001b[0;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details."
          ]
        }
      ],
      "source": [
        "report_llm = llm(response_message[0].content) #OpenAI -llms - Text davinci-003"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C011QqOIrj_5",
        "outputId": "13ef1506-264f-4933-ad7a-e6b3626e95ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User Engagement Analysis Report\n",
            "\n",
            "Key Findings:\n",
            "\n",
            "1. Active Users: The number of active users has increased by 15% over the past quarter. This growth can be attributed to the successful launch of our new mobile app, which has received positive feedback from users. The increase in active users indicates a growing user base and interest in our platform.\n",
            "\n",
            "2. Daily Logins: There has been a notable uptick in daily logins, which further supports the increase in active users. This suggests that users are finding value in our platform and are regularly accessing it to engage with the content.\n",
            "\n",
            "3. User Interactions: User interactions with key features, such as commenting and sharing, have risen by 20%. This signifies a higher level of user participation and social engagement within our platform. The increase in user interactions indicates that users are actively engaging with the content and finding it valuable enough to share and comment on.\n",
            "\n",
            "4. Average Session Durations: We have observed longer average session durations, indicating that users are spending more time exploring the content we offer. This suggests that users are finding the content engaging and are willing to spend more time on our platform.\n",
            "\n",
            "5. Engagement Rate: The engagement rate, calculated by dividing interactions by active users, has experienced a steady increase of 10% over the past two months. This suggests that our efforts to improve user experience and introduce new features are resonating well with our audience. The increase in engagement rate indicates that users are becoming more active and engaged with the platform.\n",
            "\n",
            "Overall, the data suggests that our focus on enhancing user engagement through product improvements and feature launches is yielding positive results. The increase in active users, daily logins, user interactions, average session durations, and engagement rate all indicate a growing user base and increased user engagement. As we continue to refine our strategies and innovate, we can expect further growth in user interactions and overall engagement.\n"
          ]
        }
      ],
      "source": [
        "print(report.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2bv81B92AI4",
        "outputId": "aab3b291-749b-4c2e-9344-6c434b5a3030"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'report_llm' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(report_llm)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'report_llm' is not defined"
          ]
        }
      ],
      "source": [
        "print(report_llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oockujjt2G9p"
      },
      "source": [
        "## Output in specific format and not string\n",
        "\n",
        "When developing an application, there may be a requirement for the response to adhere to a specific format, such as JSON or XML."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bdh5g6lGuZqN"
      },
      "outputs": [],
      "source": [
        "movie_review_template = \"\"\"\\\n",
        "For the following movie review, extract the following information:\n",
        "\n",
        "genre: Mentioned genres in the review. Extract and list them as a comma separated Python list.\n",
        "\n",
        "lead_chemistry: Describe the chemistry between the lead characters, if mentioned.\n",
        "\n",
        "plot_twists: Were any unexpected plot twists mentioned? Answer True if yes, False if not or unknown.\n",
        "\n",
        "runtime_feel: Did the reviewer mention their feelings about the movie's runtime? If yes, extract their sentiment about it.\n",
        "\n",
        "Format the output as JSON with the following keys:\n",
        "genre\n",
        "lead_chemistry\n",
        "plot_twists\n",
        "runtime_feel\n",
        "\n",
        "review_text: {review_text}\n",
        "\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLYLCWWH0ZO8",
        "outputId": "43442a0e-ae10-4b1e-a886-8fea04c1f623"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['review_text'], output_parser=None, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['review_text'], output_parser=None, partial_variables={}, template=\"For the following movie review, extract the following information:\\n\\ngenre: Mentioned genres in the review. Extract and list them as a comma separated Python list.\\n\\nlead_chemistry: Describe the chemistry between the lead characters, if mentioned.\\n\\nplot_twists: Were any unexpected plot twists mentioned? Answer True if yes, False if not or unknown.\\n\\nruntime_feel: Did the reviewer mention their feelings about the movie's runtime? If yes, extract their sentiment about it.\\n\\nFormat the output as JSON with the following keys:\\ngenre\\nlead_chemistry\\nplot_twists\\nruntime_feel\\n\\nreview_text: {review_text}\\n\", template_format='f-string', validate_template=True), additional_kwargs={})])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "movie_bot = ChatPromptTemplate.from_template(movie_review_template)\n",
        "movie_bot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1VyuG8u0ed_",
        "outputId": "6265faf9-3426-4b0d-9585-7fe53592e08a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['review_text']"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "movie_bot.input_variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "XMzdDTw60Jz5"
      },
      "outputs": [],
      "source": [
        "movie_review = \"\"\"\\\n",
        "This movie was an incredible cinematic experience. It offers a unique blend of genres:\\\n",
        "romance, drama, and thrilling action. The storyline is gripping, and the characters'\\\n",
        "emotional journey kept me engaged throughout. The visuals are stunning, especially\\\n",
        "during the breathtaking action sequences.\n",
        "\n",
        "The acting was exceptional, with the lead actors delivering performances that truly\\\n",
        "captured the essence of their roles. The chemistry between the main characters added\\\n",
        "depth to the romantic elements of the plot. The supporting cast also deserves praise,\\\n",
        "as they brought their characters to life convincingly.\n",
        "\n",
        "I was pleasantly surprised by the unexpected plot twists that kept me on the edge of\\\n",
        "my seat. The pacing was well-managed, allowing for a perfect balance between intense\\\n",
        "action and quieter, reflective moments.\n",
        "\n",
        "While the movie was a bit longer than usual, I found myself immersed in the story\\\n",
        "and didn't mind the extended runtime. It's a film that leaves a lasting impression,\\\n",
        "prompting viewers to reflect on its themes long after the credits roll.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "WTxO_gVh0j0J"
      },
      "outputs": [],
      "source": [
        "delegate_review = movie_bot.format_messages(\n",
        "    review_text = movie_review\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6stCe9p0wwF",
        "outputId": "8c06e108-5b43-4e32-b995-f3ed515058d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content=\"For the following movie review, extract the following information:\\n\\ngenre: Mentioned genres in the review. Extract and list them as a comma separated Python list.\\n\\nlead_chemistry: Describe the chemistry between the lead characters, if mentioned.\\n\\nplot_twists: Were any unexpected plot twists mentioned? Answer True if yes, False if not or unknown.\\n\\nruntime_feel: Did the reviewer mention their feelings about the movie's runtime? If yes, extract their sentiment about it.\\n\\nFormat the output as JSON with the following keys:\\ngenre\\nlead_chemistry\\nplot_twists\\nruntime_feel\\n\\nreview_text: This movie was an incredible cinematic experience. It offers a unique blend of genres:romance, drama, and thrilling action. The storyline is gripping, and the characters'emotional journey kept me engaged throughout. The visuals are stunning, especiallyduring the breathtaking action sequences.\\n\\nThe acting was exceptional, with the lead actors delivering performances that trulycaptured the essence of their roles. The chemistry between the main characters addeddepth to the romantic elements of the plot. The supporting cast also deserves praise,as they brought their characters to life convincingly.\\n\\nI was pleasantly surprised by the unexpected plot twists that kept me on the edge ofmy seat. The pacing was well-managed, allowing for a perfect balance between intenseaction and quieter, reflective moments.\\n\\nWhile the movie was a bit longer than usual, I found myself immersed in the storyand didn't mind the extended runtime. It's a film that leaves a lasting impression,prompting viewers to reflect on its themes long after the credits roll.\\n\\n\", additional_kwargs={}, example=False)]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "delegate_review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "tK2LXfJO0vWM"
      },
      "outputs": [],
      "source": [
        "get_movie_details = chat(delegate_review)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7OB9tys1K85",
        "outputId": "f81c2ac3-c8f0-422e-a5f7-d5fa9112c59c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"genre\": [\"romance\", \"drama\", \"thrilling action\"],\n",
            "  \"lead_chemistry\": \"The chemistry between the main characters added depth to the romantic elements of the plot.\",\n",
            "  \"plot_twists\": true,\n",
            "  \"runtime_feel\": \"I found myself immersed in the story and didn't mind the extended runtime.\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "print(get_movie_details.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGer29GG4HX3"
      },
      "source": [
        "As you can see the output is in the json format but the data type is still string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zAn8KLo1RVS",
        "outputId": "6b6c86c9-a7f1-4fa0-8f6c-7d9d204dd5c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(get_movie_details.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muDGC1K65HML"
      },
      "source": [
        "> In the code below, the .get command is used with JSON to retrieve details. However, since our response is in string format, this will result in an error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "tBdiIYMh1UQa",
        "outputId": "c6b952e2-c6af-4555-d5ae-0d487bdec53a"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-c96fa309c60a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_movie_details\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"genre\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'AIMessage' object has no attribute 'get'"
          ]
        }
      ],
      "source": [
        "get_movie_details.get(\"genre\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCGTZmOuZAm2"
      },
      "source": [
        "## Output Parser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuHTm7xw5etb"
      },
      "source": [
        "Language models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in.\n",
        "\n",
        "Output parsers are classes that help structure language model responses. There are two main methods an output parser must implement:\n",
        "\n",
        "- \"Get format instructions\": A method which returns a string containing instructions for how the output of a language model should be formatted.\n",
        "- \"Parse\": A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "ajDSkkFZuHoV"
      },
      "outputs": [],
      "source": [
        "from langchain.output_parsers import ResponseSchema\n",
        "from langchain.output_parsers import StructuredOutputParser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HBh73CO5zO0"
      },
      "source": [
        " StructuredOutputParser output parser can be used when you want to return multiple fields. While the Pydantic/JSON parser is more powerful, we initially experimented with data structures having text fields only.\n",
        "\n",
        " To create such multiple data structures we use ResponseSchema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Cd47oOJS0JqD"
      },
      "outputs": [],
      "source": [
        "genre_schema = ResponseSchema(name=\"genre\",\n",
        "                              description=\"Mentioned genres in the review. \\\n",
        "                              Extract and list them as a comma separated Python list.\")\n",
        "\n",
        "lead_chemistry_schema = ResponseSchema(name=\"lead_chemistry\",\n",
        "                                       description=\"Describe the chemistry between \\\n",
        "                                       the lead characters, if mentioned.\")\n",
        "\n",
        "plot_twists_schema = ResponseSchema(name=\"plot_twists\",\n",
        "                                    description=\"Were any unexpected plot twists mentioned? \\\n",
        "                                    Answer True if yes, False if not or unknown.\")\n",
        "\n",
        "runtime_feel_schema = ResponseSchema(name=\"runtime_feel\",\n",
        "                                     description=\"Did the reviewer mention their feelings about \\\n",
        "                                     the movie's runtime? If yes, extract their sentiment about it.\")\n",
        "\n",
        "response_schemas = [genre_schema,\n",
        "                    lead_chemistry_schema,\n",
        "                    plot_twists_schema,\n",
        "                    runtime_feel_schema]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "MUBGGppv12eq"
      },
      "outputs": [],
      "source": [
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWnwhP-P16kl",
        "outputId": "b9f9bcf4-e02e-44d8-fe5a-b524987661e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"genre\": string  // Mentioned genres in the review.                               Extract and list them as a comma separated Python list.\n",
            "\t\"lead_chemistry\": string  // Describe the chemistry between                                        the lead characters, if mentioned.\n",
            "\t\"plot_twists\": string  // Were any unexpected plot twists mentioned?                                     Answer True if yes, False if not or unknown.\n",
            "\t\"runtime_feel\": string  // Did the reviewer mention their feelings about                                      the movie's runtime? If yes, extract their sentiment about it.\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "format_instructions = output_parser.get_format_instructions()\n",
        "print(format_instructions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFx0HtUJ58pm"
      },
      "source": [
        "> Note: Use this format_instructions in your prompt template to get the response in JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "zWNf7Vdw297O"
      },
      "outputs": [],
      "source": [
        "movie_review_template = \"\"\"\\\n",
        "For the following movie review, extract the following information:\n",
        "\n",
        "genre: Mentioned genres in the review. Extract and list them as a comma separated Python list.\n",
        "\n",
        "lead_chemistry: Describe the chemistry between the lead characters, if mentioned.\n",
        "\n",
        "plot_twists: Were any unexpected plot twists mentioned? Answer True if yes, False if not or unknown.\n",
        "\n",
        "runtime_feel: Did the reviewer mention their feelings about the movie's runtime? If yes, extract their sentiment about it.\n",
        "\n",
        "Format the output as JSON with the following keys:\n",
        "genre\n",
        "lead_chemistry\n",
        "plot_twists\n",
        "runtime_feel\n",
        "\n",
        "review_text: {review_text}\n",
        "\n",
        "{format_instructions}\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "hkgdV8Sg3Cov"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_template(template=movie_review_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "Lo8Sc5YK3JoB"
      },
      "outputs": [],
      "source": [
        "user_review = prompt.format_messages(\n",
        "    review_text=movie_review,\n",
        "    format_instructions=format_instructions\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "-SqcS5pn3cSs"
      },
      "outputs": [],
      "source": [
        "response = chat(user_review)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKzt8qfI3g46",
        "outputId": "5c05f7f2-239a-4f8c-8d60-8d6ffddd2d46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```json\n",
            "{\n",
            "\t\"genre\": [\"romance\", \"drama\", \"thrilling action\"],\n",
            "\t\"lead_chemistry\": \"The chemistry between the main characters added depth to the romantic elements of the plot.\",\n",
            "\t\"plot_twists\": true,\n",
            "\t\"runtime_feel\": \"While the movie was a bit longer than usual, I found myself immersed in the story and didn't mind the extended runtime.\"\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "tFiYdTEg3oJB"
      },
      "outputs": [],
      "source": [
        "final_ans = output_parser.parse(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWRMnN4-3t22",
        "outputId": "99342c11-0597-47bb-bbcc-bddc3cdd2acd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'genre': ['romance', 'drama', 'thrilling action'],\n",
              " 'lead_chemistry': 'The chemistry between the main characters added depth to the romantic elements of the plot.',\n",
              " 'plot_twists': True,\n",
              " 'runtime_feel': \"While the movie was a bit longer than usual, I found myself immersed in the story and didn't mind the extended runtime.\"}"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIoZLfCD3vYu",
        "outputId": "970f67d7-3baf-40d3-f507-ee7e376a671f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['romance', 'drama', 'thrilling action']"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_ans.get(\"genre\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZTs-eQlZCU8"
      },
      "source": [
        "## Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "v4E8lJeluDQb"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.chains import SequentialChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "yffDaLET4lde"
      },
      "outputs": [],
      "source": [
        "movie_review_template = \"\"\"\\\n",
        "For the following movie review, extract the following information:\n",
        "\n",
        "genre: Mentioned genres in the review. Extract and list them as a comma separated Python list.\n",
        "\n",
        "lead_chemistry: Describe the chemistry between the lead characters, if mentioned.\n",
        "\n",
        "plot_twists: Were any unexpected plot twists mentioned? Answer True if yes, False if not or unknown.\n",
        "\n",
        "runtime_feel: Did the reviewer mention their feelings about the movie's runtime? If yes, extract their sentiment about it.\n",
        "\n",
        "Format the output as JSON with the following keys:\n",
        "genre\n",
        "lead_chemistry\n",
        "plot_twists\n",
        "runtime_feel\n",
        "\n",
        "review_text: {review_text}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "rhaPHT4Z4pTu"
      },
      "outputs": [],
      "source": [
        "user_prompt = ChatPromptTemplate.from_template(movie_review_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "eH_xZKgr4-YQ"
      },
      "outputs": [],
      "source": [
        "chat = ChatOpenAI(temperature=0.0) #ChatOPENAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdbFtBlA5n3s"
      },
      "source": [
        "## LLM Chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vp23y8o56fZd"
      },
      "source": [
        "> Note:\n",
        ">\n",
        "> verbose = True, shows the logs that happens in the background\n",
        ">\n",
        "> verbose = False, displays no logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5fDdj0u37UR",
        "outputId": "fad99805-cf3b-4110-e881-08df28a1a4b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mHuman: For the following movie review, extract the following information:\n",
            "\n",
            "genre: Mentioned genres in the review. Extract and list them as a comma separated Python list.\n",
            "\n",
            "lead_chemistry: Describe the chemistry between the lead characters, if mentioned.\n",
            "\n",
            "plot_twists: Were any unexpected plot twists mentioned? Answer True if yes, False if not or unknown.\n",
            "\n",
            "runtime_feel: Did the reviewer mention their feelings about the movie's runtime? If yes, extract their sentiment about it.\n",
            "\n",
            "Format the output as JSON with the following keys:\n",
            "genre\n",
            "lead_chemistry\n",
            "plot_twists\n",
            "runtime_feel\n",
            "\n",
            "review_text: This movie was an incredible cinematic experience. It offers a unique blend of genres:romance, drama, and thrilling action. The storyline is gripping, and the characters'emotional journey kept me engaged throughout. The visuals are stunning, especiallyduring the breathtaking action sequences.\n",
            "\n",
            "The acting was exceptional, with the lead actors delivering performances that trulycaptured the essence of their roles. The chemistry between the main characters addeddepth to the romantic elements of the plot. The supporting cast also deserves praise,as they brought their characters to life convincingly.\n",
            "\n",
            "I was pleasantly surprised by the unexpected plot twists that kept me on the edge ofmy seat. The pacing was well-managed, allowing for a perfect balance between intenseaction and quieter, reflective moments.\n",
            "\n",
            "While the movie was a bit longer than usual, I found myself immersed in the storyand didn't mind the extended runtime. It's a film that leaves a lasting impression,prompting viewers to reflect on its themes long after the credits roll.\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "chain = LLMChain(llm=chat,prompt=user_prompt,verbose=True) # by default - Verbose=False\n",
        "review_res = chain.run(movie_review)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3xtk6No5QUr",
        "outputId": "7f766e0c-3e36-45ef-a5d9-be0bcc0f6d8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"genre\": [\"romance\", \"drama\", \"thrilling action\"],\n",
            "  \"lead_chemistry\": \"The chemistry between the main characters added depth to the romantic elements of the plot.\",\n",
            "  \"plot_twists\": true,\n",
            "  \"runtime_feel\": \"I found myself immersed in the story and didn't mind the extended runtime.\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "print(review_res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saNMWaH95t83"
      },
      "source": [
        "## SequentialChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "Kl_Hz2jm7D-C"
      },
      "outputs": [],
      "source": [
        "chain_one = LLMChain(llm=chat,prompt=user_prompt,output_key = \"json_answer\") #outputkey can be any name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "M5XaWtTL43_Q"
      },
      "outputs": [],
      "source": [
        "summary_review = \"Please summarize the following review in 1 liner: {review_text}\"\n",
        "second_prompt_template = ChatPromptTemplate.from_template(summary_review)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "Zo2u3PAv79tI"
      },
      "outputs": [],
      "source": [
        "chain_two = LLMChain(llm =chat,prompt=second_prompt_template,output_key = \"summary\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "Xh47BGp28IND"
      },
      "outputs": [],
      "source": [
        "seq_chain = SequentialChain(\n",
        "    chains=[chain_one, chain_two],\n",
        "    input_variables=[\"review_text\"],\n",
        "    output_variables=[\"json_answer\", \"summary\"],\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOvw3okD9NxS",
        "outputId": "7693b36b-1ee1-44d2-beb7-2ab3182f7395"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "seq_response = seq_chain(movie_review)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-Q5yBWh9mf8",
        "outputId": "149ebbc8-6d25-4818-8707-ae7a0c0cc64e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'review_text': \"This movie was an incredible cinematic experience. It offers a unique blend of genres:romance, drama, and thrilling action. The storyline is gripping, and the characters'emotional journey kept me engaged throughout. The visuals are stunning, especiallyduring the breathtaking action sequences.\\n\\nThe acting was exceptional, with the lead actors delivering performances that trulycaptured the essence of their roles. The chemistry between the main characters addeddepth to the romantic elements of the plot. The supporting cast also deserves praise,as they brought their characters to life convincingly.\\n\\nI was pleasantly surprised by the unexpected plot twists that kept me on the edge ofmy seat. The pacing was well-managed, allowing for a perfect balance between intenseaction and quieter, reflective moments.\\n\\nWhile the movie was a bit longer than usual, I found myself immersed in the storyand didn't mind the extended runtime. It's a film that leaves a lasting impression,prompting viewers to reflect on its themes long after the credits roll.\\n\",\n",
              " 'json_answer': '{\\n  \"genre\": [\"romance\", \"drama\", \"thrilling action\"],\\n  \"lead_chemistry\": \"The chemistry between the main characters added depth to the romantic elements of the plot.\",\\n  \"plot_twists\": true,\\n  \"runtime_feel\": \"I found myself immersed in the story and didn\\'t mind the extended runtime.\"\\n}',\n",
              " 'summary': 'This movie is an incredible cinematic experience with a unique blend of genres, gripping storyline, stunning visuals, exceptional acting, unexpected plot twists, and a lasting impression.'}"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "seq_response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9qCcV76ZPEN"
      },
      "source": [
        "## Memory\n",
        "\n",
        "![lanchain_memory](https://python.langchain.com/assets/images/memory_diagram-0627c68230aa438f9b5419064d63cbbc.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "y2HfomJzuIo_"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "JPVhMaEi-094"
      },
      "outputs": [],
      "source": [
        "temp1 = ChatPromptTemplate.from_template(\"Hi, my name is {name}, I work at AI Planet\")\n",
        "temp1 = temp1.format_messages(name=\"Tarun Jain\")\n",
        "res = chat(temp1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9SNFERv_BCR",
        "outputId": "725a361a-c966-4534-8145-77e174fe4779"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Hello Tarun Jain! Nice to meet you. How can I assist you today?', additional_kwargs={}, example=False)"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "fW5hGiebAND5"
      },
      "outputs": [],
      "source": [
        "temp2 = ChatPromptTemplate.from_template(\"As you know I work at {startup}. What is my name?\")\n",
        "temp2 = temp2.format_messages(startup=\"AI Planet\")\n",
        "res2 = chat(temp2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SI53HIjgAmYM",
        "outputId": "64356c57-edcd-4e4c-c6af-5f51fa9da554"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"I'm sorry, but as an AI language model, I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation. I am designed to respect user privacy and confidentiality. Therefore, I don't know your name unless you explicitly tell me. My primary function is to provide information and answer questions to the best of my knowledge and abilities. If you have any concerns about privacy or data security, please let me know, and I will do my best to address them.\", additional_kwargs={}, example=False)"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AzdM39rAu7L"
      },
      "source": [
        "## Memory\n",
        "\n",
        "Most LLM applications have a conversational interface. An essential component of a conversation is being able to refer to information introduced earlier in the conversation. At bare minimum, a conversational system should be able to access some window of past messages directly. A more complex system will need to have a world model that it is constantly updating, which allows it to do things like maintain information about entities and their relationships."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "7uWdn_gHv-Ta"
      },
      "outputs": [],
      "source": [
        "memory = ConversationBufferMemory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "sPbo3QNv-S9r"
      },
      "outputs": [],
      "source": [
        "conversation = ConversationChain(\n",
        "    llm=chat,\n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "JMM7ASwd-Wx1",
        "outputId": "77ebe5f6-3aad-4cc6-d142-fe14d2de9407"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi, my name is Tarun Jain, I work at AI Planet\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Hello Tarun Jain! It's nice to meet you. I'm an AI and I'm here to chat with you. How can I assist you today?\""
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"Hi, my name is Tarun Jain, I work at AI Planet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "b6e91rE8-kdf",
        "outputId": "cb4c2c55-a779-4c5d-bb81-5c7258c0f5ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Tarun Jain, I work at AI Planet\n",
            "AI: Hello Tarun Jain! It's nice to meet you. I'm an AI and I'm here to chat with you. How can I assist you today?\n",
            "Human: At AI Planet I work as a DevRel & Community Manager\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"That's great! As a DevRel & Community Manager, your role is to build and maintain relationships with developers and the community. You likely organize events, create content, and provide support to developers using AI technologies. It's an important role in fostering a strong developer community and driving adoption of AI solutions. Is there anything specific you would like to discuss or any questions you have about your role?\""
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"At AI Planet I work as a DevRel & Community Manager\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "l-VNHCgHAxPk",
        "outputId": "3bc0bf73-eb95-474f-bf25-00f388f2ca52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Tarun Jain, I work at AI Planet\n",
            "AI: Hello Tarun Jain! It's nice to meet you. I'm an AI and I'm here to chat with you. How can I assist you today?\n",
            "Human: At AI Planet I work as a DevRel & Community Manager\n",
            "AI: That's great! As a DevRel & Community Manager, your role is to build and maintain relationships with developers and the community. You likely organize events, create content, and provide support to developers using AI technologies. It's an important role in fostering a strong developer community and driving adoption of AI solutions. Is there anything specific you would like to discuss or any questions you have about your role?\n",
            "Human: What is my name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Your name is Tarun Jain.'"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"What is my name?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTQC4NlxA5yW",
        "outputId": "4bb005ff-da6b-4627-ba99-667aca28b59e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Human: Hi, my name is Tarun Jain, I work at AI Planet\n",
            "AI: Hello Tarun Jain! It's nice to meet you. I'm an AI and I'm here to chat with you. How can I assist you today?\n",
            "Human: At AI Planet I work as a DevRel & Community Manager\n",
            "AI: That's great! As a DevRel & Community Manager, your role is to build and maintain relationships with developers and the community. You likely organize events, create content, and provide support to developers using AI technologies. It's an important role in fostering a strong developer community and driving adoption of AI solutions. Is there anything specific you would like to discuss or any questions you have about your role?\n",
            "Human: What is my name?\n",
            "AI: Your name is Tarun Jain.\n"
          ]
        }
      ],
      "source": [
        "print(memory.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71cd6L-w7EY9"
      },
      "source": [
        "## Agents\n",
        "\n",
        "Some applications will require not just a predetermined chain of calls to LLMs/other tools, but potentially an unknown chain that depends on the user's input. In these types of chains, there is a “agent” which has access to a suite of tools. Depending on the user input, the agent can then decide which, if any, of these tools to call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOFHJA_Y8PeY",
        "outputId": "ce9fa3a8-740e-410f-ba5c-e756c6628cfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting duckduckgo-search\n",
            "  Downloading duckduckgo_search-3.8.5-py3-none-any.whl (18 kB)\n",
            "Collecting aiofiles>=23.1.0 (from duckduckgo-search)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from duckduckgo-search) (8.1.7)\n",
            "Requirement already satisfied: lxml>=4.9.2 in /usr/local/lib/python3.10/dist-packages (from duckduckgo-search) (4.9.3)\n",
            "Collecting httpx[brotli,http2,socks]>=0.24.1 (from duckduckgo-search)\n",
            "  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx[brotli,http2,socks]>=0.24.1->duckduckgo-search) (2023.7.22)\n",
            "Collecting httpcore<0.18.0,>=0.15.0 (from httpx[brotli,http2,socks]>=0.24.1->duckduckgo-search)\n",
            "  Downloading httpcore-0.17.3-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx[brotli,http2,socks]>=0.24.1->duckduckgo-search) (3.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx[brotli,http2,socks]>=0.24.1->duckduckgo-search) (1.3.0)\n",
            "Collecting socksio==1.* (from httpx[brotli,http2,socks]>=0.24.1->duckduckgo-search)\n",
            "  Downloading socksio-1.0.0-py3-none-any.whl (12 kB)\n",
            "Collecting brotli (from httpx[brotli,http2,socks]>=0.24.1->duckduckgo-search)\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2<5,>=3 (from httpx[brotli,http2,socks]>=0.24.1->duckduckgo-search)\n",
            "  Downloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hyperframe<7,>=6.0 (from h2<5,>=3->httpx[brotli,http2,socks]>=0.24.1->duckduckgo-search)\n",
            "  Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
            "Collecting hpack<5,>=4.0 (from h2<5,>=3->httpx[brotli,http2,socks]>=0.24.1->duckduckgo-search)\n",
            "  Downloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore<0.18.0,>=0.15.0->httpx[brotli,http2,socks]>=0.24.1->duckduckgo-search)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx[brotli,http2,socks]>=0.24.1->duckduckgo-search) (3.7.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx[brotli,http2,socks]>=0.24.1->duckduckgo-search) (1.1.3)\n",
            "Installing collected packages: brotli, socksio, hyperframe, hpack, h11, aiofiles, httpcore, h2, httpx, duckduckgo-search\n",
            "Successfully installed aiofiles-23.2.1 brotli-1.1.0 duckduckgo-search-3.8.5 h11-0.14.0 h2-4.1.0 hpack-4.0.0 httpcore-0.17.3 httpx-0.24.1 hyperframe-6.0.1 socksio-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install duckduckgo-search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "BiEsn8BQ7fEi"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import Tool\n",
        "from langchain.agents import AgentType\n",
        "from langchain.utilities import DuckDuckGoSearchAPIWrapper\n",
        "from langchain.agents import initialize_agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "dMh-UNFW70vd"
      },
      "outputs": [],
      "source": [
        "search = DuckDuckGoSearchAPIWrapper()\n",
        "tools = [\n",
        "    Tool(\n",
        "        name = \"Search Engine\",\n",
        "        func=search.run,\n",
        "        description=\"To explore the world of internet\"\n",
        "    ),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "HV5fgIew8UAJ"
      },
      "outputs": [],
      "source": [
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "llm = ChatOpenAI(openai_api_key=openai_key, temperature=0.0)\n",
        "agent_chain = initialize_agent(tools, llm, agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION, verbose=True, memory=memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D00FnXls8jVs"
      },
      "source": [
        "### Agent Type: Do refer the official documentation [docs](https://python.langchain.com/docs/modules/agents/agent_types/)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "pl5F00fx8tVv",
        "outputId": "6be1b97a-bd9b-45c2-b0cf-354e38335303"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m{\n",
            "    \"action\": \"Search Engine\",\n",
            "    \"action_input\": \"Location of Bangalore\"\n",
            "}\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mIt lies 3,113 feet (949 metres) above sea level, atop an east-west ridge in the Karnataka Plateau in the southeastern part of the state, at a cultural meeting point of the Kannada -, Telugu -, and Tamil -speaking peoples. Pop. (2001) city, 4,301,326; urban agglom., 5,701,446; (2011) city, 8,443,675; urban agglom., 8,520,435. History MapsofIndia.com - Map showing the location of Bangalore,Karnataka in India. Find where is Bangalore located. Bengaluru Railway Map. Bengaluru or Bangalore is the capital of Karnataka and one of the most prominent cities of India. It has a population of 8,443,675 (2011 census) and ranks as the third ... About Map: Map showing location of Bangalore in the state of Karnatak, India Where is Bengaluru Located? Bangalore officially Bengaluru is the capital and largest city of the Indian state of Karnataka. Bengaluru lies between 12°58′44″ North latitudes and 77°35′30° East longitudes. The city covers an area of about 741 km2 with a ... Bangaluru Location Map, Karnataka. Click here for Customized Maps. Print. Email. Free Download. Buy Now. * Map showing the location of Bangalore in Karnataka. Disclaimer: All efforts have been ...\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"Bangalore is located in the southeastern part of the state of Karnataka, India. It is situated atop an east-west ridge in the Karnataka Plateau, at an elevation of 3,113 feet (949 meters) above sea level. Bangalore is a cultural meeting point for the Kannada, Telugu, and Tamil-speaking peoples.\"\n",
            "}\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Bangalore is located in the southeastern part of the state of Karnataka, India. It is situated atop an east-west ridge in the Karnataka Plateau, at an elevation of 3,113 feet (949 meters) above sea level. Bangalore is a cultural meeting point for the Kannada, Telugu, and Tamil-speaking peoples.'"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_chain.run(input=\"Where is Bangalore located?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Otjsw1wc7Iex"
      },
      "source": [
        "## Reference:\n",
        "\n",
        "- [AIWithTarun Video](https://www.youtube.com/watch?v=IbMtSXTJ0ic)\n",
        "- [Langchain documentation](https://docs.langchain.com/docs/)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
